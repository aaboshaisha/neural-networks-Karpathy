{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e9f09e3-866f-4379-845e-eb19c6c2b6bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "Useful links for practice:\n",
    "- [Python + Numpy tutorial from CS231n](https://cs231n.github.io/python-numpy)... . We use torch.tensor instead of numpy.array in this video. Their design (e.g. broadcasting, data types, etc.) is so similar that practicing one is basically practicing the other, just be careful with some of the APIs - how various functions are named, what arguments they take, etc. - these details can vary.\n",
    "- [PyTorch tutorial on Tensor](https://pytorch.org/tutorials/beginne)...\n",
    "- [Another PyTorch intro to Tensor](https://pytorch.org/tutorials/beginne)..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa74284-a7a2-43d6-9691-e0295dbaef1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be588108-ce9e-464f-8873-999ae48751b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### E01: \n",
    "Train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ddc498-a764-4f4c-8d2e-b5bc6dd11214",
   "metadata": {},
   "source": [
    "#### Ans\n",
    "First I'll attempt the solution using counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6f7faeda-91ab-4d06-8482-428584189be3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "words = open('/Users/amralaa/Desktop/nbs/zero2hero/names.txt').read().splitlines()\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd3b2f8-0574-43a0-acad-d5666fc77eee",
   "metadata": {
    "tags": []
   },
   "source": [
    "We want first to be able to extract trigrams from the data. We'll put them in a dict and keep counts of occurences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dd61bb85-98a6-4e4c-925e-f6a301f315ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = {}\n",
    "\n",
    "for w in words[:1]:\n",
    "    w = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(w,w[1:],w[2:]):\n",
    "        #print(f'{ch1+ch2} {ch3}')\n",
    "        t[(ch1+ch2, ch3)] = t.get((ch1+ch2, ch3), 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "111cbb4d-c945-4a58-b872-eb390b3c02f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([(('.e', 'm'), 1), (('em', 'm'), 1), (('mm', 'a'), 1), (('ma', '.'), 1)])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7f4fbf69-ed56-4a26-8d8b-e7f244000ef1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('.e', 'm'), 1), (('em', 'm'), 1), (('mm', 'a'), 1), (('ma', '.'), 1)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can sort them by frequency same as we did in the lecture\n",
    "sorted(t.items(), key=lambda kv:-kv[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1f8f4b-2c9e-4a06-bdcb-9cebfa2ac533",
   "metadata": {},
   "source": [
    "The `dict.items()` method returns a view object. The view object is a list containing the key-value pairs of the dictionary, as tuples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562647a9-5f79-4e16-a010-1406af055074",
   "metadata": {},
   "source": [
    "We then need to build a table (as torch tensor) which has 1st 2 ccs as rows and 3rd cc as columns such that each cell will give counts of how many times this single cc follows that pair of ccs.\n",
    "\n",
    "We have 27 ccs. The number of pairs is 27 Choose 2 = 351. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "baab9fa5-c7ad-4018-86c0-d0691eb640bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4d8e2adf-340a-4599-bf37-03a16b0dfff4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "351"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.comb(27, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c0ab91-6424-4054-8f58-23a5660aea4a",
   "metadata": {},
   "source": [
    "To do the sampling, we'll use `itertools.combinations`\n",
    "\n",
    "`itertools.combinations(iterable, r)` --> combinations('ABCD', 2) --> AB AC AD BC BD CD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fd2c37e6-5eba-425e-86ae-5e25f1b0d059",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c006306f-0b69-4ef3-ab70-db3bbe412de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first, get unique characters\n",
    "\n",
    "chars = ['.'] + sorted(list(set(''.join(words))))\n",
    "chars[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "feed0399-e57b-4a32-83aa-3ade96afe61f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "351"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get pairs -> confirm length == 351\n",
    "len(list(combinations(chars, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b0fac897-969d-49dc-ac43-73150629ce58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pchars = list(combinations(chars, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30de7a66-8e02-4107-9377-fd6881bd1eac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a52206-5c31-4ea2-9c8a-66423d7c1d73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7a0234-4db3-4fe4-a789-8453e19bfd57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124244dd-313e-4c11-8a2e-f31214a769fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bfecb8-0e68-484c-9680-a9d056b9a9cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a65490f7-1653-4ea0-9f5a-4518d7ad7916",
   "metadata": {
    "tags": []
   },
   "source": [
    "### E02: \n",
    "Split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807f9b6b-ae57-4997-a58b-7c0f0d9abf26",
   "metadata": {
    "tags": []
   },
   "source": [
    "### E03: \n",
    "Use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767b5ecf-1a84-4d5c-b79e-4893f4fe2282",
   "metadata": {
    "tags": []
   },
   "source": [
    "### E04: \n",
    "We saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eece74a7-97fb-4ca7-a9dc-df0c7ced9770",
   "metadata": {
    "tags": []
   },
   "source": [
    "### E05: \n",
    "Look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9715c4e5-37a6-4389-8fe5-ef04127db325",
   "metadata": {
    "tags": []
   },
   "source": [
    "### E06: \n",
    "Meta-exercise! Think of a fun/interesting exercise and complete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b07744-f593-40ef-a6f9-1934e07e88a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4ff866-6cda-4ff5-8e67-c85c0002260c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
