{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e9f09e3-866f-4379-845e-eb19c6c2b6bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "Useful links for practice:\n",
    "- [Python + Numpy tutorial from CS231n](https://cs231n.github.io/python-numpy)... . We use torch.tensor instead of numpy.array in this video. Their design (e.g. broadcasting, data types, etc.) is so similar that practicing one is basically practicing the other, just be careful with some of the APIs - how various functions are named, what arguments they take, etc. - these details can vary.\n",
    "- [PyTorch tutorial on Tensor](https://pytorch.org/tutorials/beginne)...\n",
    "- [Another PyTorch intro to Tensor](https://pytorch.org/tutorials/beginne)..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa74284-a7a2-43d6-9691-e0295dbaef1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be588108-ce9e-464f-8873-999ae48751b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### E01: \n",
    "Train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ddc498-a764-4f4c-8d2e-b5bc6dd11214",
   "metadata": {},
   "source": [
    "#### Ans\n",
    "First I'll attempt the solution using counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6f7faeda-91ab-4d06-8482-428584189be3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "words = open('/Users/amralaa/Desktop/nbs/zero2hero/names.txt').read().splitlines()\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd3b2f8-0574-43a0-acad-d5666fc77eee",
   "metadata": {
    "tags": []
   },
   "source": [
    "We want first to be able to extract trigrams from the data. We'll put them in a dict and keep counts of occurences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dd61bb85-98a6-4e4c-925e-f6a301f315ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = {}\n",
    "pairs = [] #get pairs of characters as we'll need later\n",
    "\n",
    "for w in words:\n",
    "    w = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(w,w[1:],w[2:]):\n",
    "        #print(f'{ch1+ch2} {ch3}')\n",
    "        t[(ch1+ch2, ch3)] = t.get((ch1+ch2, ch3), 0) + 1\n",
    "        if (ch1+ch2) not in pairs:\n",
    "            pairs.append(ch1+ch2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "111cbb4d-c945-4a58-b872-eb390b3c02f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#t.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bf532db9-b3fe-47d0-a349-4c5cd605d582",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "601"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7f4fbf69-ed56-4a26-8d8b-e7f244000ef1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we can sort them by frequency same as we did in the lecture\n",
    "#sorted(t.items(), key=lambda kv:-kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9913acbd-3fde-4029-9839-4a1bd7965141",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "counts = sorted(t.items(), key=lambda kv:-kv[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1f8f4b-2c9e-4a06-bdcb-9cebfa2ac533",
   "metadata": {},
   "source": [
    "The `dict.items()` method returns a view object. The view object is a list containing the key-value pairs of the dictionary, as tuples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562647a9-5f79-4e16-a010-1406af055074",
   "metadata": {},
   "source": [
    "We then need to build a table (as torch tensor) which has 1st 2 ccs as rows and 3rd cc as columns such that each cell will give counts of how many times this single cc follows that pair of ccs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c006306f-0b69-4ef3-ab70-db3bbe412de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get unique characters\n",
    "chars = ['.'] + sorted(list(set(''.join(words))))\n",
    "chars[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e8da2c87-587b-42c8-b5a6-c5d381de886a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi_c = {}\n",
    "for ix, ch in enumerate(chars):\n",
    "    stoi_c[ch] = ix\n",
    "#stoi_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b2d1da0a-0a8b-4f10-aebd-d001cff6c314",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stoi_p = {}\n",
    "for ix, pr in enumerate(pairs):\n",
    "    stoi_p[pr] = ix\n",
    "#stoi_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cfe28934-b673-454b-ae3b-75d2b06e43bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "45f6cae6-2868-4ca4-ac46-129b2fc5266d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([601, 27])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now let's create the table / array / tensor\n",
    "N = torch.zeros((len(pairs), len(chars)), dtype=torch.int32)\n",
    "N.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5ff7684e-83a5-4df0-9333-1754c44c9ef0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now, let's populate it with counts\n",
    "for w in words:\n",
    "    w = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(w, w[1:], w[2:]):\n",
    "        ix1 = stoi_p[ch1+ch2]\n",
    "        ix2 = stoi_c[ch3]\n",
    "        N[ix1, ix2] += 1\n",
    "        #print((ch1+ch2, ch3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ac632f4a-7229-4915-a678-ae9e8ec91ed1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "itos_c = {v:k for k,v in stoi_c.items()}\n",
    "itos_p = {v:k for k,v in stoi_p.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c87c8e2c-a444-49be-b109-43544c8e4b92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([601, 27])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eecb20-1d27-4594-a753-45479dd032f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now, we have a table where each row represents a bigram and each column represents how many times a single letter follows that bigram in the dataset.\n",
    "\n",
    "Remeber we want to make a model that given a bigram can predict the next letter that follows and can generate new names this way. In this model, we could for example start with \".e\" and the model predict \"m\" then we have \"em\" and the model predicts \"a\" and so on. \n",
    "\n",
    "The question the model is asking each time then: given I see \".e\", what is the probability I get \"a\" or \"b\" or \"c\" ..etc.\n",
    "So, given a bigram, we want a probability distribution over the single characters. We use it to sample from them.\n",
    "But, we have counts not probabilities. We thus need to generate probabilities from counts.\n",
    "\n",
    "We have 601 x 27 tensor. We want a probabilty distribution over the single letters (columns) for each of the 601 bigrams (rows). So, we want to divide each cell in a row by the sum of that row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "68e77ba2-084d-4fc1-9480-b4559a090243",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([601, 1])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get row sums\n",
    "N.sum(1, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "e3384a26-694c-4b14-abb7-fd8832aa998c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "P = N / N.sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8c836d8e-e836-4228-8ef6-f6a42802e599",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confirm probability worked out\n",
    "p.sum(1, keepdim=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "37e87c77-4159-44a1-9d71-a23e616e2169",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a generator for the sampling\n",
    "g = torch.Generator().manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4c5d06-fc2e-4f2b-9c80-b4ff47435ead",
   "metadata": {
    "tags": []
   },
   "source": [
    "How to sample? \n",
    "\n",
    "We have rows of bigrams \".a, .b, .c, ... aa, ab, .. , zz\"\\\n",
    "We have cols of singles \"a, b, c .. .\"\\\n",
    "The only possible start bigrams are \".a, .b, ...z\"\\\n",
    "```\n",
    "We need to initialize our draw by sampling from the possible 27 \".letter\" bigrams out of the 601 bigram rows.\n",
    "Suppose we pull \".e\", we then get its row and sample the next single letter from it eg \"m\"\n",
    "Our bigram then becomes \"em\" and we gets its row and sample a single letter from it.\n",
    "We keep going until we hit \".\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d29c44f-7e85-4af3-938e-0a5d2dc38c6a",
   "metadata": {
    "tags": []
   },
   "source": [
    "The Multinomial distribution is a generalization of the Binomial.\\\n",
    "Binomial counts the successes in a fixed number of trials that can only be categorized as success or failure.\\\n",
    "Multinomial keeps track of trials whose outcomes can fall into multiple categories, such as excellent, adequate, poor; or red, yellow, green, blue. (From:introduction-to-probability-by-joseph-k-blitzstein-and-jessica-hwang)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "16695850-9158-4352-ac6d-2d59f42b29f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# frequencies of .letter bigrams (possible start bigrams)\n",
    "alphabet = sorted(list(set(''.join(words))))\n",
    "starts = {}\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        if ch1 == '.':\n",
    "            starts[ch1+ch2] = starts.get(ch1+ch2, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "1e8f9d64-def8-47b1-921b-64092851b1e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#for tup in sorted(starts.items()):\n",
    "    #print(tup[0], tup[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "40e72509-8ea1-4a9e-bc53-99a8232c079a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Nstarts = torch.tensor([tup[1] for tup in sorted(starts.items())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9449a448-47f6-44b6-91f0-537305685ca9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pstarts = Nstarts / Nstarts.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e7d3af40-8054-49e5-a7d3-c21156bef473",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1377, 0.0408, 0.0481, 0.0528, 0.0478, 0.0130, 0.0209, 0.0273, 0.0184,\n",
       "        0.0756, 0.0925, 0.0491, 0.0792, 0.0358, 0.0123, 0.0161, 0.0029, 0.0512,\n",
       "        0.0642, 0.0408, 0.0024, 0.0117, 0.0096, 0.0042, 0.0167, 0.0290])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pstarts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "50065a5d-7230-4ce7-8043-c6a0a6310173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create conversion dicts for these \n",
    "itos_bi = {i:'.'+ltr for i, ltr in enumerate(alphabet)}\n",
    "#itos_bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "ba85f009-b4f0-4609-a56c-fcda84c5f71b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.a'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we can sample from starts and find out which bigram we sampled\n",
    "ixr = torch.multinomial(pstarts, num_samples=1, replacement=True, generator=g).item()\n",
    "bigram = itos_bi[ixr]\n",
    "bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "2ffb9abf-1a15-4e42-86a5-f879c2c4f99c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# then we can get the row of that bigram\n",
    "single = stoi_p[bigram] #next single\n",
    "single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "67f75c42-e1ba-4271-ab40-d164d2c8663b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.a'"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm we have the right bigram\n",
    "itos_p[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "2a779598-8a48-4950-aaaf-763c1c045347",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0469, 0.0431, 0.0070, 0.0830, 0.0125, 0.0048, 0.0039, 0.0206,\n",
       "        0.0349, 0.0061, 0.0170, 0.1433, 0.0871, 0.1413, 0.0023, 0.0039, 0.0020,\n",
       "        0.1093, 0.0440, 0.0163, 0.0345, 0.0551, 0.0014, 0.0061, 0.0392, 0.0345])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get its row\n",
    "psingle = P[single]\n",
    "psingle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "8f452bde-78d7-48a8-82a7-0fa8c80f1b95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample from it\n",
    "ixc = torch.multinomial(psingle, num_samples=1, replacement=True, generator=g).item()\n",
    "ixc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "d35f67d4-7276-4b13-8797-d4e8f6fe675d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d'"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mono = itos_c[ixc]\n",
    "mono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "7c607be5-f634-453b-95ff-b2589553d03f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ad'"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram[1] + mono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "83fefdb1-1459-45fb-98b4-4bdc74154443",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# next bigram & repeat\n",
    "stoi_p[bigram[1] + mono]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "4b2f8c3e-d7ff-4ec5-b6d9-c95b8210bbc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caychann.\n",
      "khuni.\n",
      "dey.\n",
      "xon.\n",
      "ke.\n",
      "emada.\n",
      "maily.\n",
      "tee.\n",
      "helee.\n",
      "za.\n",
      "ka.\n",
      "belleytene.\n",
      "anidex.\n",
      "ertyani.\n",
      "juldynarshep.\n",
      "joulrow.\n",
      "keanveen.\n",
      "mah.\n",
      "gwynni.\n",
      "elysesinakyia.\n"
     ]
    }
   ],
   "source": [
    "# in a loop\n",
    "for _ in range(20):\n",
    "    \n",
    "    ixbi = torch.multinomial(pstarts, num_samples=1, replacement=True, generator=g).item() # ix of starting bigram\n",
    "    bi = itos_bi[ixbi] # the bigram itself\n",
    "    out = [bi[1]] \n",
    "\n",
    "    while True:\n",
    "        ixr = stoi_p[bi] # pull the row of the bigram\n",
    "        p = P[ixr] # distribution over single letters following the bigram\n",
    "        ixc = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item() # sample from it\n",
    "        mono = itos_c[ixc] # get the single letter to add to output\n",
    "        out.append(mono)\n",
    "        bi = bi[1]+mono # new bigram (move 1 letter in prev bigram + add new single)\n",
    "        if mono == '.':\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4f6513-e383-400c-8787-1b40a3f7142d",
   "metadata": {},
   "source": [
    "**How to evaluate the quality of this model**\n",
    "\n",
    "Let's compare the frequency counts (probabilities) of our model to the random model where any single character could follow any bigram.\\\n",
    "If any single character could follow any bigram, what do we expect the probability of that to be? \n",
    "\n",
    "?? is it 1/ num bigrams = 1 / 601 ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "9c095979-bab9-495c-a369-e28ddaa4d3b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "em , m : 0.1300390064716339\n",
      "mm , a : 0.4285714328289032\n",
      "ol , i : 0.11147011071443558\n",
      "li , v : 0.02177419327199459\n",
      "iv , i : 0.2899628281593323\n",
      "vi , a : 0.16136114299297333\n",
      "av , a : 0.19304555654525757\n",
      "is , a : 0.10790273547172546\n",
      "sa , b : 0.06328059732913971\n",
      "ab , e : 0.319778174161911\n",
      "be , l : 0.3068702220916748\n",
      "el , l : 0.253078818321228\n",
      "ll , a : 0.25055763125419617\n",
      "so , p : 0.03954802080988884\n",
      "op , h : 0.38947367668151855\n",
      "ph , i : 0.29901960492134094\n",
      "hi , a : 0.1111111119389534\n"
     ]
    }
   ],
   "source": [
    "for w in words[:5]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(w, w[1:], w[2:]):\n",
    "        ixr = stoi_p[ch1+ch2]\n",
    "        ixc = stoi_c[ch3]\n",
    "        print(f'{ch1+ch2} , {ch3} : {P[ixr, ixc]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3338dd4-be61-4ded-b6aa-9c2af3c277d3",
   "metadata": {},
   "source": [
    "### An Exploration of Negative log-likelihood:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24360142-2391-4e2f-8307-1b3d7c22a16d",
   "metadata": {},
   "source": [
    "Can we summarize all of these probabilities into one number? Each one of these is the probability that our model (the table) produces each one of these (bigram, monogram) tuples. What is the probability that our model produces all these examples (samples)? Well, multiply all these probabilities together. We call the resulting number the likelihood.\\\n",
    "Since multiplying probabilities (floats between 0 and 1) is problematic, we'll change this to a summation of log(probs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "d2d09757-da0d-4024-a464-20d29d836c44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "em , m : 0.1300390064716339 , -2.0399208068847656\n",
      "mm , a : 0.4285714328289032 , -0.8472978472709656\n",
      "ol , i : 0.11147011071443558 , -2.1939988136291504\n",
      "li , v : 0.02177419327199459 , -3.8270297050476074\n",
      "iv , i : 0.2899628281593323 , -1.2380025386810303\n",
      "vi , a : 0.16136114299297333 , -1.8241102695465088\n",
      "av , a : 0.19304555654525757 , -1.6448290348052979\n",
      "is , a : 0.10790273547172546 , -2.226525068283081\n",
      "sa , b : 0.06328059732913971 , -2.760176420211792\n",
      "ab , e : 0.319778174161911 , -1.1401277780532837\n",
      "be , l : 0.3068702220916748 , -1.1813303232192993\n",
      "el , l : 0.253078818321228 , -1.3740543127059937\n",
      "ll , a : 0.25055763125419617 , -1.3840663433074951\n",
      "so , p : 0.03954802080988884 , -3.2302396297454834\n",
      "op , h : 0.38947367668151855 , -0.9429590106010437\n",
      "ph , i : 0.29901960492134094 , -1.20724618434906\n",
      "hi , a : 0.1111111119389534 , -2.1972246170043945\n",
      "-31.259136199951172\n"
     ]
    }
   ],
   "source": [
    "log_likelihood = 0\n",
    "for w in words[:5]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(w, w[1:], w[2:]):\n",
    "        ixr = stoi_p[ch1+ch2]\n",
    "        ixc = stoi_c[ch3]\n",
    "        p = P[ixr, ixc]\n",
    "        logp = torch.log(p)\n",
    "        log_likelihood += logp\n",
    "        print(f'{ch1+ch2} , {ch3} : {p} , {logp}')\n",
    "print(log_likelihood.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "598faca2-661d-42e2-b945-773baace6943",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean negative log-likelihood : 2.2954144641680614\n"
     ]
    }
   ],
   "source": [
    "#Turn to positive number and take the mean. Do for all words\n",
    "n = 0 #num probs added\n",
    "log_likelihood = 0\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(w, w[1:], w[2:]):\n",
    "        ixr = stoi_p[ch1+ch2]\n",
    "        ixc = stoi_c[ch3]\n",
    "        p = P[ixr, ixc]\n",
    "        logp = torch.log(p)\n",
    "        n += 1\n",
    "        log_likelihood += logp\n",
    "        #print(f'{ch1+ch2} , {ch3} : {p} , {logp}')\n",
    "\n",
    "nll = - log_likelihood.item()\n",
    "nll /= n\n",
    "print(f'Mean negative log-likelihood : {nll}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f267b341-dbbd-4e08-8d8a-620cb3876c4a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Using a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b29c46-53b8-4a9d-9079-163a235ed398",
   "metadata": {},
   "source": [
    "How do we train a neural network to do the above? We want the neural network to learn the frequency counts by itself from the data. It takes as input a bigram and outputs a likely monogram following it.\\\n",
    "Each pair of (bigram, monogram) will be an example to the NN.\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "d7fdf41c-cab8-444a-9976-f7ba50c69d2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(601, 27)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs), len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "dbf1eb15-8f02-4cfc-9f03-9995125768d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xs = []\n",
    "ys = []\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        xs.append(stoi_p[ch1+ch2])\n",
    "        ys.append(stoi_c[ch3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "46d369e2-e645-451b-ac90-24a1d06631ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xs) == len(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "3e557855-f0c5-4095-8fa1-39fe013876cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert to tensors to provide to NN\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "cee962ed-0318-449b-b4fc-a2346af5f7b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([196113]), torch.Size([196113]))"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.shape, ys.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d922bcb2-3df1-4ac9-ba5d-067b7bb79a8c",
   "metadata": {},
   "source": [
    "But these are indices which we can't provide to a neural network. We will need to do one-hot-encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "12cf4089-0761-4308-ab58-73fc338376a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xenc = torch.nn.functional.one_hot(xs).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "59ea6e90-6888-4847-b460-bf986c757b89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([196113, 601]), torch.float32)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.shape, xenc.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05fe302-c143-4dd4-afc2-419ea7afcaa0",
   "metadata": {},
   "source": [
    "We now get 196113 sample inputs. Each sample is encoded in 601 digits (so will need at least 601 synapses). We try to predict the distribution over the 27 single characters that can follow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "45761afa-d3a8-46ef-af24-1cbc6e5a8a99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "W = torch.randn((xenc.shape[1], 27), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "0dd1a7cb-d03f-466e-a432-cf02c0439f23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([196113, 27])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xenc@W).shape # 1 prediction per example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "c1df9403-cd82-4763-be24-eba255eb425b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13, 13,  1,  0, 12,  9, 22,  9,  1,  0])"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "e3b5c899-8926-4d07-abba-dc1a3371871f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.4593, 0.8345, 1.3124,  ..., 1.7834, 1.4012, 1.0623],\n",
       "        [0.3151, 0.3909, 0.1857,  ..., 2.1218, 0.6410, 3.7147],\n",
       "        [0.9335, 0.9316, 3.8462,  ..., 4.0080, 0.1569, 1.3187],\n",
       "        ...,\n",
       "        [0.7967, 1.4419, 0.2656,  ..., 1.1188, 2.4957, 0.2041],\n",
       "        [3.3804, 0.5675, 0.5530,  ..., 0.2170, 0.5539, 0.8496],\n",
       "        [0.8514, 0.6238, 0.9864,  ..., 0.6190, 3.3779, 0.8521]],\n",
       "       grad_fn=<ExpBackward0>)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xenc@W).exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5566501d-59ae-4737-b8d9-cafd30fc97da",
   "metadata": {},
   "source": [
    "We interpret the numbers the network produces as log(counts) aka logits. We therefore can get counts from them using exp(logits). Once we have counts, we can get probabilities. When we have probabilities, we can get negative log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "eb0c6898-2d5d-4d60-b5a9-5341dfa07502",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([196113, 27])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = (xenc@W)\n",
    "counts = logits.exp()\n",
    "counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "003e7452-0c8b-4901-81fb-385b036d21ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([196113, 27])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = counts / counts.sum(dim=1, keepdim=True)\n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "afd2756c-a573-4859-af15-e50be9afd887",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1771, 0.0229, 0.0360, 0.0155, 0.0726, 0.0321, 0.0412, 0.0644, 0.0318,\n",
       "        0.0257, 0.0090, 0.0139, 0.0239, 0.0132, 0.0173, 0.0151, 0.0061, 0.0410,\n",
       "        0.0238, 0.0032, 0.0036, 0.0529, 0.0620, 0.0795, 0.0489, 0.0384, 0.0291],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "71f583a4-12af-4422-a88b-cdd2f08c4ec9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "22892624-c57d-42a3-b226-4834141d83ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013177163898944855"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what is the probability of the right next character (ie prob of ys) \n",
    "probs[0][ys[0]].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "a82ac98a-0f99-4034-a6d7-fe1ebc860757",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196113"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(probs[torch.arange(ys.shape[0]), ys]) # to extract all the probabilties of the true ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "58c7e229-550c-49b9-88c6-fabb44375c54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss = - probs[torch.arange(ys.shape[0]), ys].log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "32536c38-edcb-4936-a920-e8f2c60df47f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.8069190979003906"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f61f09f-60e6-4c5d-9472-bbbc37e41abc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "7e80174e-08b9-4df2-8dd0-207d8f3029b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196113"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89553e8-3094-48cc-bbd8-5829f487d268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68d74a3-f78f-4b57-8feb-f06b9982d305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a65490f7-1653-4ea0-9f5a-4518d7ad7916",
   "metadata": {
    "tags": []
   },
   "source": [
    "### E02: \n",
    "Split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807f9b6b-ae57-4997-a58b-7c0f0d9abf26",
   "metadata": {
    "tags": []
   },
   "source": [
    "### E03: \n",
    "Use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767b5ecf-1a84-4d5c-b79e-4893f4fe2282",
   "metadata": {
    "tags": []
   },
   "source": [
    "### E04: \n",
    "We saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eece74a7-97fb-4ca7-a9dc-df0c7ced9770",
   "metadata": {
    "tags": []
   },
   "source": [
    "### E05: \n",
    "Look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9715c4e5-37a6-4389-8fe5-ef04127db325",
   "metadata": {
    "tags": []
   },
   "source": [
    "### E06: \n",
    "Meta-exercise! Think of a fun/interesting exercise and complete it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
