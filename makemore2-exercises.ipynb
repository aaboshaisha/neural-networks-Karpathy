{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c28202c3-daf3-4e84-bcf5-b95d0423c299",
   "metadata": {},
   "source": [
    "### Exercises: Building makemore Part 2: MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3bc8bf-d8f8-44cf-8a2f-4b7e7b0fd830",
   "metadata": {},
   "source": [
    "- E01: Tune the hyperparameters of the training to beat my best validation loss of 2.2\n",
    "- E02: I was not careful with the intialization of the network in this video. (1) What is the loss you'd get if the predicted probabilities at initialization were perfectly uniform? What loss do we achieve? (2) Can you tune the initialization to get a starting loss that is much more similar to (1)?\n",
    "- E03: Read the Bengio et al 2003 paper (link above), implement and try any idea from the paper. Did it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "714f3516-20d8-4313-932f-ad365e442a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e21d44d-e0a1-4d5b-8189-0b36a7594722",
   "metadata": {},
   "source": [
    "### E01: \n",
    "Tune the hyperparameters of the training to beat my best validation loss of 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "535041e0-7032-4f63-953b-d7a75b631216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4643cff4-b36e-412c-831a-2553af8d29ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd', 'e']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "chars[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "908d6bfc-3baf-41da-abb1-7b74110a481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1017754c-5b4d-4bbb-84ee-856953846c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create datasets\n",
    "block_size = 3\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    \n",
    "    for word in words:\n",
    "        context = [0] * block_size\n",
    "        \n",
    "        for ch in word+'.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "    \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af559945-508c-49b9-9430-305456430031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "accfac2e-783a-486c-aabc-05555fa97167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into train / dev / test 80/10/10\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words)) # index for 80 - 90 \n",
    "n2 = int(0.9*len(words)) # index for 90 - 100\n",
    "\n",
    "xtr, ytr = build_dataset(words[:n1])\n",
    "xdev, ydev = build_dataset(words[n1:n2])\n",
    "xte, yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5ca3a279-092f-44e3-9c7e-c55b48066920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([182580, 3]),\n",
       " torch.Size([182580]),\n",
       " torch.Size([22767, 3]),\n",
       " torch.Size([22767]),\n",
       " torch.Size([22799, 3]),\n",
       " torch.Size([22799]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtr.shape, ytr.shape, xdev.shape, ydev.shape, xte.shape, yte.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "5679808f-735e-447a-ba37-0f5c328a3857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize embedding layer, weights and biases\n",
    "feature_size = 10 #size of feature vector\n",
    "C = torch.randn((27, feature_size))\n",
    "emb = C[xtr].view(-1, block_size * feature_size)\n",
    "hidden_size = 200\n",
    "W1 = torch.randn((block_size * feature_size, hidden_size), requires_grad = True)\n",
    "b1 = torch.randn(hidden_size, requires_grad = True)\n",
    "W2 = torch.randn((hidden_size, 27), requires_grad = True)\n",
    "b2 = torch.randn(27, requires_grad = True)\n",
    "\n",
    "parameters = [W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "0f923351-e061-41d7-93dc-a67ed4357bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, num_iters, lr, bs):\n",
    "    # keep stats\n",
    "    lossi = []\n",
    "    iteration = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        ixs = torch.randint(0, X.shape[0], (bs, ))\n",
    "        # forward\n",
    "        hidden = torch.tanh((C[X[ixs]].view(-1, block_size * feature_size) @ W1 + b1))\n",
    "        logits = hidden @ W2 + b2\n",
    "        loss = F.cross_entropy(logits, Y[ixs])\n",
    "    \n",
    "        # backward\n",
    "        for p in parameters: # zero grads\n",
    "            p.grad = None\n",
    "        loss.backward() #backprop\n",
    "\n",
    "        #lr = 0.1 if i < 10000 else lr\n",
    "        if i < 100000:\n",
    "            lr = 0.1\n",
    "        elif ((i > 100000) and (i < 150000)):\n",
    "            lr = 0.01\n",
    "        else:\n",
    "            lr = lr\n",
    "        \n",
    "        for p in parameters: # weight update\n",
    "            p.data -= lr * p.grad\n",
    "\n",
    "        lossi.append(loss.item())\n",
    "        iteration.append(i)\n",
    "    \n",
    "    return iteration, lossi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "805a4e88-76a1-450b-a694-15ca01786c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loss(x, y):\n",
    "    hidden = torch.tanh(C[x].view(-1, block_size * feature_size) @ W1 + b1)\n",
    "    logits = hidden @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y).item()\n",
    "    print(f'Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "0f5c884e-1a83-4748-b676-0b8d535a16a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "i, lossi = train(xtr, ytr, 200000, 0.001, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "ab4edf63-e933-4432-87f9-dc3f3acaf150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.190006732940674"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lossi[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "c643365e-98c5-406b-aca8-47e702d1ff14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.0791778564453125\n"
     ]
    }
   ],
   "source": [
    "eval_loss(xtr, ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "44b95d9e-264f-4f36-b3eb-0c6bdf309bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.1441211700439453\n"
     ]
    }
   ],
   "source": [
    "eval_loss(xdev, ydev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fd5924-b824-4220-b5bb-3e6ba36ff8a7",
   "metadata": {},
   "source": [
    "Managed to beat Karpathy's score. I doubled the batch size to 64, iterated for 200,000 times and varied the learning rate during the iterations such that the network uses a slower learning rate the more iterations it goes through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e06b9ed-5291-4e95-a390-d5830ac99ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
