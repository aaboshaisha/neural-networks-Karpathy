{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rToK0Tku8PPn"
   },
   "source": [
    "## makemore: becoming a backprop ninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8sFElPqq8PPp"
   },
   "outputs": [],
   "source": [
    "# there no change change in the first several cells from last lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ChBbac4y8PPq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "x6GhEWW18aCS"
   },
   "outputs": [],
   "source": [
    "# download the names.txt file from github\n",
    "#!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "klmu3ZG08PPr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "BCQomLE_8PPs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "V_zt2QHr8PPs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):\n",
    "  X, Y = [], []\n",
    "\n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "eg20-vsg8PPt"
   },
   "outputs": [],
   "source": [
    "# ok biolerplate done, now we get to the action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "MJPU8HT08PPu"
   },
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ZlFLjQyT8PPu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "# Note: I am initializating many of these parameters in non-standard ways\n",
    "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass.\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "QY-y96Y48PPv"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # a shorter variable also, for convenience\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8ofj1s6d8PPv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3403, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "\n",
    "emb = C[Xb] # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "  p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "  t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**logprobs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs[range(n), Yb].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`loss = -logprobs[range(n), Yb].mean()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is loss computing?\n",
    "\n",
    "`logprobs` has shape (32 x 27)\\\n",
    "`logprobs[range(n), Yb]` : uses range(n) to go through each row, then plucks 1 column out of 27 from each row that corresponds to the index given by `Yb`\\\n",
    "These 32 values will be the ones used in the numercial calculation of the mean as `[x1, x2, ...., x32].mean()`. We want to calculate the gradient of the loss function w.r.t each one of these. What will the formula for the gradient be? \n",
    "\n",
    "Smaller example to get the numerical form:\n",
    "```\n",
    "loss = (x1 + x2 + x3) / 3\n",
    "y = f(x) = [x1, x2, x3].mean()\n",
    "f(x) = (x1 + x2 + x3) / 3\n",
    "df/dx1 = 1/3\n",
    "df/dx2 = 1/3\n",
    "df/dx3 = 1/3\n",
    "dy = [1/3, 1/3, 1/3]dx\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the rest of the values that were not \"plucked\" and so not used in the computation of loss?\\\n",
    "If they were not part of the caculation of loss, then their derivative must be 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlogprobs = torch.zeros(logprobs.shape)\n",
    "dlogprobs[range(n), Yb] = -1/n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogprobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Andrej's code\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n), Yb] = -1.0/n "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dprobs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`logprobs = probs.log()` \\\n",
    "This is a log function that takes probs as input and outputs logprobs. ie: `y = f(x) = log(x)`\\\n",
    "The local drrivative of this operation is `dy/dx = 1/x`\\\n",
    "And remeber from our implementation of micrograd : this multiplies the backpropagated derivative from the loss function (in this case it's `dlogprobs`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dprobs = (1.0/probs) * dlogprobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is `dprobs` doing?\\\n",
    "`dprobs = (1.0/probs) * dlogprobs`\\\n",
    "`(1.0/probs)` is the local derivative. `dlogprobs` is the derivative propagated via chain rule.\\\n",
    "`(1.0/probs)`: if probs is 1 or close to 1 (ie ur model is predicting the correct category), then this division's result will be 1 or a small number and `dlogprobs` gradient will just flow back as is or shrunk. If probs are small however (ur model is making the wrong predictions), then this will be a big number and will inflate the `dlogprobs` gradient (asking the model to make bigger changes since it's too far off). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dcounts_sum_inv**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about this math expression: `probs = counts * counts_sum_inv`\n",
    "\n",
    "This is like `c = a x b` and we want to backpropagate through `c` so we have to get the derivative w.r.t `a` and derivative w.r.t `b`.\\\n",
    "If usign scalars, this is easy enough. `dc/da = b` and `dc/db = a`. How to do it for matrices?\\\n",
    "Here, we have to be careful about the shapes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32, 1]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.shape, counts_sum_inv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`probs = counts * counts_sum_inv`\n",
    " is the same line as\\\n",
    "`probs = counts / counts.sum()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice there are 2 operations happening here:\n",
    "- One is the broadcasting (replication) of the 1 column of `counts_sum_inv` across 27 columns.\n",
    "- Two is the element-wise division occuring.\n",
    "\n",
    "So, we'll do our gradient calculation in 2 steps as well:\n",
    "\n",
    "```\n",
    "c = a x b with tensors will be\n",
    "a(3x3) * b(3x1)\n",
    "a11 a12 a13   b1\n",
    "a21 a22 a23   b2\n",
    "a31 a32 a33   b3\n",
    "\n",
    "When broadcast\n",
    "\n",
    "a11*b1  a12*b1  a13*b1  \n",
    "a21*b2  a22*b2  a23*b2\n",
    "a31*b3  a32*b3  a33*b3\n",
    "\n",
    "And this becomes c(3x3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, `c` is `probs`, `a` is `counts` and `b` is `counts_sum_inv`\n",
    "\n",
    "We want `dprobs / dcounts_sum_inv` ie `dc/db` which in this case is `a` or `counts`. Notice, this is the derivative w.r.t the \"replicated\" `b`.\n",
    "\n",
    "But now, we need to backpropagate through the replication. We can see for eg `b1` is the same variable being used multiple times. We saw with autograd that if a variable (single node) is used multiple times (feeds into multiple nodes), we need to sum all the gradients that come from all of its uses. \n",
    "\n",
    "Since `b` is used in multiple columns, the right thing to do is to sum across them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcounts_sum_inv = (counts * dprobs).sum(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dcounts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32, 1]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.shape, counts_sum_inv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcounts = counts_sum_inv * dprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcounts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you check `cmp('counts', dcounts, counts)` now you'll get an error.Why? Look at the code:\n",
    "```\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1\n",
    "probs = counts * counts_sum_inv\n",
    "```\n",
    "\n",
    "U can see we backpropagated from `counts` to `probs`. But, `counts_sum_inv` also uses `counts` in `counts_sum_inv = counts_sum**-1` so we still have to resolve this branch before we get the correct result (we still have to calculate the second contribution). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dcounts_sum**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "counts_sum_inv = counts_sum**-1\n",
    "dcounts_sum_inv / dcounts_sum = - counts_sum**-2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcounts_sum = -counts_sum**-2 * dcounts_sum_inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dcounts**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`counts_sum = counts.sum(1, keepdims=True)`\n",
    "\n",
    "`counts_sum` results from a sum operation over `counts`\n",
    "First, look at the shapes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1]), torch.Size([32, 27]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_sum.shape, counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "c1 c2 c3\n",
    "c4 c5 c6\n",
    "c7 c8 c9\n",
    "\n",
    "Sum across columns to give (cs:count_sum , c:counts)\n",
    "\n",
    "cs1 (c1+c2+c3) \n",
    "cs2 (c4+c5+c6)\n",
    "cs3 (c7+c8+c9)\n",
    "\n",
    "dcs1/dc1 = 1 + 0 + 0\n",
    "\n",
    "Same for else\n",
    "\n",
    "Now, the drrivative of cs1 will depend on c1,c2,c3 but won't depenend on c4,c5...c9. So, derivative of cs1 w.r.t c1 will be 1 - also for c2 and c3 - but will be 0 for all else.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be multiplied by the derivative flowing backwards from the chain rule. But we have to be careful because we already calculated `dcounts` from a previous branch so we need to make sure these branches add up (use `+=`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcounts += torch.ones_like(counts) * dcounts_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dnorm_logits**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`counts = norm_logits.exp()`\n",
    "\n",
    "We want:\n",
    "\n",
    "`dcounts / dnorm_logits`. `counts` here results from an element-wise exponentiation process `exp(norm_logits)` as if we're doing:\n",
    "```\n",
    "exp ( x1 x2 x3\n",
    "      x4 x5 x6\n",
    "      x7 x8 x9 )\n",
    "```\n",
    "\n",
    "We know derivative of exp is exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnorm_logits = norm_logits.exp() * dcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dlogit_maxes & dlogits** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want: `dnorm_logits / dlogit_maxes`\n",
    "\n",
    "`norm_logits = logits - logit_maxes # subtract max for numerical stability`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1]), torch.Size([32, 27]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_maxes.shape, logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "c = a - b\n",
    "dc/da = 1\n",
    "dc/db = -1\n",
    "\n",
    "How to do it for matrices?\n",
    "\n",
    "max ( x1 x2 x3\n",
    "      x4 x5 x6\n",
    "      x7 x8 x9 )\n",
    "\n",
    "suppose b = \n",
    "x2\n",
    "x4\n",
    "x9\n",
    "\n",
    "c = \n",
    "x1-x2  x2-x2  x3-x2\n",
    "x4-x4  x5-x4  x6-x4\n",
    "x7-x9  x8-x9  x9-x9\n",
    "\n",
    "=\n",
    "\n",
    "dc/da =\n",
    "1 1 1\n",
    "1 1 1\n",
    "1 1 1\n",
    "\n",
    "dc/db =\n",
    "\n",
    "-1 -1 -1\n",
    "-1 -1 -1\n",
    "-1 -1 -1\n",
    "```\n",
    "\n",
    "If these local gradients are -1, 1, then they will allow the backpropagated gradient `dnorm_logits` to flow backwards as they are (except reversing the sign for dlogit_maxes).\\\n",
    "Also, keep in mind, for `db => dlogit_maxes`, we replicated / broadcast it to do the subtraction, so we need to sum back the gradients across the dimnsion we replicated (columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlogits = dnorm_logits.clone()\n",
    "dlogit_maxes = -dnorm_logits.sum(dim=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remeber, `logits` is also used in another branch `logit_maxes = logits.max(1, keepdim=True).values`. We need to compute it before we get the final correct derivative for it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A note on `dlogit_maxes`:\n",
    "\n",
    "Remeber where it came from: `norm_logits = logits - logit_maxes # subtract max for numerical stability`\n",
    "\n",
    "Recall the discussion on numerical stability. We subtract `logit_maxes` from `logits` (subtract max of each row from the cell in that row). This is sort of normalizing to prevent arbitrarily high numbers from logits which could mess up the calculations later when we do exp and log and such resulting in floating point problems. But, since we subtract equally from all values in each row, this subtracted value `logit_maxes` should have no impact on `probs` or the final loss. We can double check that by looking at their gradients. We expect it to be 0 or very small values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.5611e-09],\n",
       "        [-3.7253e-09],\n",
       "        [-1.8626e-09],\n",
       "        [ 1.3970e-09],\n",
       "        [-4.6566e-10],\n",
       "        [ 2.3283e-10],\n",
       "        [-0.0000e+00],\n",
       "        [ 2.7940e-09],\n",
       "        [ 3.7253e-09],\n",
       "        [ 4.6566e-10],\n",
       "        [-9.3132e-10],\n",
       "        [ 4.6566e-10],\n",
       "        [-1.8626e-09],\n",
       "        [-3.7253e-09],\n",
       "        [-1.8626e-09],\n",
       "        [-6.9849e-10],\n",
       "        [-4.1910e-09],\n",
       "        [-9.3132e-10],\n",
       "        [-3.7253e-09],\n",
       "        [-2.3283e-09],\n",
       "        [-3.7253e-09],\n",
       "        [-7.2177e-09],\n",
       "        [ 3.7253e-09],\n",
       "        [ 3.2596e-09],\n",
       "        [ 1.8626e-09],\n",
       "        [ 1.6298e-09],\n",
       "        [-1.3970e-09],\n",
       "        [-2.7940e-09],\n",
       "        [ 1.8626e-09],\n",
       "        [-1.8626e-09],\n",
       "        [-1.8626e-09],\n",
       "        [ 1.1642e-09]], grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogit_maxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we see this is indeed the case. In implementation we might actually choose not to backpropagate through this branch and set its derivative to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dlogits**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From : `logit_maxes = logits.max(1, keepdim=True).values`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices, values = torch.tensor([[1,2],[4,3],[5,6]]).max(dim=1,keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2],\n",
       "         [4],\n",
       "         [6]]),\n",
       " tensor([[1],\n",
       "         [0],\n",
       "         [1]]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices, values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "c = max(a, b) \n",
    "\n",
    "\n",
    "max ( x1 x2\n",
    "      x3 x4\n",
    "      x5 x6 )\n",
    "\n",
    "suppose b = \n",
    "x2\n",
    "x4\n",
    "x5\n",
    "\n",
    "what we get is\n",
    "\n",
    "x2 = max(x1, x2)\n",
    "x4 = max(x3, x4)\n",
    "x5 = max(x5, x6)\n",
    "\n",
    "And intuitively, if we change x1, it should have no impact on x2 so derivative = 0 or dx2/dx1 = 0 , while dx2/dx2 = 1\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_indices = logits.max(dim=1).indices # to represent as list\n",
    "dlogits2 = torch.zeros_like(logits)\n",
    "dlogits2[torch.arange(logits.shape[0]), max_indices] = 1\n",
    "plt.imshow(dlogits2);\n",
    "dlogits += (dlogits2 * dlogit_maxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Andrej's code\n",
    "\n",
    "# we need to run these 2 lines to reset the code\n",
    "dlogits = dnorm_logits.clone()\n",
    "dlogit_maxes = -dnorm_logits.sum(dim=1, keepdims=True)\n",
    "\n",
    "# then the new branch\n",
    "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dh, dW2, db2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`logits = h @ W2 + b2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`$$\\begin{bmatrix} 1 & 2 & 1 \\\\ 3 & 0 & 1 \\\\ 0 & 2 & 4 \\end{bmatrix}$$`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{bmatrix} 1 & 2 & 1 \\\\ 3 & 0 & 1 \\\\ 0 & 2 & 4 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finish converting the handwritten notes to latex notes here later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long story short: we find that backward pass of matrix multiplication is a matrix multiplication.\n",
    "```\n",
    "if: d = a @ b + c, and loss is L such that we have dL / dd\n",
    "dL/da = dL/dd @ b.T\n",
    "dL/db = a.T @ dL/dd\n",
    "dL/dc = dL/dd.sum(0)\n",
    "```\n",
    "In both cases, it's like the scalar case where the final derivative is the backpropagaed derivative x the other term in the multiplication (in the case of `a` and `b`), and for `c` it's a sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Hacky Way**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, do we need to remember these formulas? Do we need to derive them every time?\\\n",
    "No. We can get the right results by remebering a couple of things: \n",
    "1. The result will be some sort of matrix multiplication of the backpropagated loss by one of the terms transposed (in the case of multiplication) or it's the backpropagated loss summed across some dimension in case of bias term.\n",
    "2. The shapes of the matrices must work out.\n",
    "\n",
    "What does that mean? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have `logits = h @ W2 + b2`. We want: `dh, dW2 and db2`. Their shapes of these derivatives must correspond to the matrices they're supposed to be the derivative of. ie shape of `dh` same as `h`, `dW2` same as `W2` and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]),\n",
       " torch.Size([32, 64]),\n",
       " torch.Size([64, 27]),\n",
       " torch.Size([27]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits.shape, h.shape, W2.shape, b2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we start with `dh`, it has to be 32 x 64. And we know we get it via matrix multiplying `dlogits` (32 x 27) by `W2` (64 x 27). The only way to get this is if we do `dlogits @ W2.T` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dh = dlogits @ W2.T\n",
    "dh.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And indeed we get that.\n",
    "Let's do the same for the others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 27])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dW2 = h.T @ dlogits\n",
    "dW2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db2 = dlogits.sum(0)\n",
    "db2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dhpreact**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`h = torch.tanh(hpreact)`\n",
    "\n",
    "Remeber the formula from micrograd:\n",
    "\n",
    "if $$t = tanh(x) = \\frac{exp(2x - 1)}{exp(2x + 1)} $$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$ \\frac{dt}{dx} = \\frac{dtanh(x)}{dx} = 1 - t^2 $$\n",
    "\n",
    "Keep in mind the derivative is in terms of the output of tanh -> t not the input x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpreact.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dhpreact = (1 - h**2) * dh\n",
    "dhpreact.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dbngain, dbnraw, dbnbias**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`hpreact = bngain * bnraw + bnbias` \n",
    "\n",
    "We want:\n",
    "\n",
    "```\n",
    "dhpreact / dbngain\n",
    "dhpreact / dbnraw\n",
    "dhpreact / dbnbias\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple case because we have element-wise multiplication not matrix multiplication so it will be same as doing derivative for:\n",
    "\n",
    "```\n",
    "d = a * b + c\n",
    "dd/da = b\n",
    "dd/db = a\n",
    "dd/dc = 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]),\n",
       " torch.Size([1, 64]),\n",
       " torch.Size([32, 64]),\n",
       " torch.Size([1, 64]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dhpreact.shape, bngain.shape, bnraw.shape, bnbias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we just need to be careful about the shapes because there is broadcasting going on so whenever a row is replicated vertically, we need to sum the final result across rows (dim=0) and so on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbngain = (bnraw * dhpreact).sum(dim=0, keepdim=True)\n",
    "dbnraw =  bngain * dhpreact\n",
    "dbnbias = dhpreact.sum(dim=0, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backpropagating through the BatchNorm Layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dbnvar_inv, dbndiff**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bnraw = bndiff * bnvar_inv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]), torch.Size([32, 64]), torch.Size([1, 64]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as usual, check the shapes first\n",
    "bnraw.shape, bndiff.shape , bnvar_inv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is element-wise so it's easy\n",
    "dbndiff = dbnraw * bnvar_inv\n",
    "dbndiff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbnvar_inv = (dbnraw * bndiff).sum(dim=0, keepdim=True) \n",
    "dbnvar_inv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next `bnvar_inv = (bnvar + 1e-5)**-0.5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64]), torch.Size([1, 64]), torch.Size([32, 64]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the shapes\n",
    "dbnvar_inv.shape, bnvar.shape, dbnraw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbnvar = ((-0.5 * (bnvar + 1e-5)**-1.5) * dbnvar_inv).sum(dim=0, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next: `bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64]), torch.Size([1, 64]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnvar.shape, bndiff2.sum(0, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toy example:\n",
    "\n",
    "```\n",
    "(a11 a12\n",
    " a21 a22).sum(0, ..) \n",
    "\n",
    "* (1 / n-1)\n",
    "\n",
    "-->\n",
    "\n",
    "b1 = 1/(n-1) * (a11 + a21) # sum over rows then scale by 1/n-1\n",
    "b2 = 1/(n-1) * (a12 + a22)\n",
    "\n",
    "db / da =\n",
    "\n",
    "[ db1/da11  db1/da12\n",
    "  db2/da21  db2/db22 ]\n",
    "\n",
    "=>\n",
    "db1/da11 = 1/(n-1) * (1 + 0)\n",
    "db1/da12 = 1/(n-1) * (1 + 0)\n",
    "db2/da21 = 1/(n-1) * (1 + 0)\n",
    "db2/db22 = 1/(n-1) * (1 + 0)\n",
    "\n",
    "=\n",
    "\n",
    "[ 1 1\n",
    "  1 1 ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbndiff2 = (torch.ones_like(bndiff2) * (1.0 / (n-1))) * dbnvar\n",
    "dbndiff2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next `bndiff2 = bndiff**2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]), torch.Size([32, 64]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the shapes\n",
    "bndiff2.shape, bndiff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add the old branch to reset every time we run this\n",
    "dbndiff = dbnraw * bnvar_inv\n",
    "# then the new one\n",
    "dbndiff += 2*bndiff * dbndiff2\n",
    "dbndiff.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next `bndiff = hprebn - bnmeani`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]), torch.Size([32, 64]), torch.Size([1, 64]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the shapes\n",
    "dbndiff.shape, hprebn.shape, bnmeani.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "c = a - b\n",
    "dc/da = 1 - 0\n",
    "dc/db = 0 - 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64]), torch.Size([32, 64]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the subtraction is a simple case\n",
    "dhprebn  =   dbndiff.clone()\n",
    "dbnmeani =  -dbndiff.clone().sum(dim=0, keepdim=True)\n",
    "dbnmeani.shape, dhprebn.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next: `bnmeani = 1/n*hprebn.sum(0, keepdim=True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is similar to a previous case \n",
    "\n",
    "```\n",
    "(a11 a12\n",
    "a21 a22).sum(0, ...)\n",
    "\n",
    "x\n",
    "\n",
    "1/n\n",
    "\n",
    "-->\n",
    "\n",
    "b1 = 1/n * (a11 + a21)\n",
    "b2 = 1/n * (a12 + a22)\n",
    "\n",
    "db / da -->\n",
    "\n",
    "[ db1/da11 db1/da21\n",
    "  db2/da12 db2/da22 ]\n",
    "\n",
    "[1 1\n",
    " 1 1] * 1/n\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64]), torch.Size([32, 64]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnmeani.shape, hprebn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dhprebn  =   dbndiff.clone() # run previous branch\n",
    "\n",
    "dhprebn += ((1/n) * dbnmeani) * torch.ones_like(hprebn) # torch.ones_like(hprebn): to replicate to compensate for the sum\n",
    "dhprebn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final layers:\n",
    "\n",
    "```\n",
    "emb = C[Xb] # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next: `hprebn = embcat @ W1 + b1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is similar to the matrix layer we did before. Let's see if we can do it the \"Hacky\" way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]),\n",
       " torch.Size([32, 30]),\n",
       " torch.Size([30, 64]),\n",
       " torch.Size([64]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the shapes\n",
    "hprebn.shape, embcat.shape, W1.shape, b1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 30])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we know dembcat needs to be 32 x 30 and it results from matrix multipying dhprebn (32 x 64) and W1 (30 X 64) someway\n",
    "dembcat = dhprebn @ W1.T\n",
    "dembcat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 64])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we know dW1 needs to be 30 x 64 and it results from matrix multipying dhprebn (32 x 64) and embcat (32 x 30) somehow\n",
    "dW1 = embcat.T @ dhprebn\n",
    "dW1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fianlly db1 will be 1s multiplied by hprebn 32 x 64 but collapsed to 64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db1 = dhprebn.clone().sum(dim=0)\n",
    "db1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it works :)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next: `embcat = emb.view(emb.shape[0], -1)`\n",
    "\n",
    "We have the derivative of `embcat` and we want to backpropagate to `emb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 30]), torch.Size([32, 3, 10]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embcat.shape, emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "emb is embcat (so same gradient) but with different shape. We just need to reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "demb = dembcat.view(emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally: `emb = C[Xb]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 10]) torch.Size([27, 10]) torch.Size([32, 3])\n",
      "tensor([[ 1,  1,  4],\n",
      "        [18, 14,  1],\n",
      "        [11,  5,  9],\n",
      "        [ 0,  0,  1],\n",
      "        [12, 15, 14]])\n"
     ]
    }
   ],
   "source": [
    "print(emb.shape, C.shape, Xb.shape)\n",
    "print(Xb[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the derivative of `emb`, we want to backpropagate to `C`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remind ourselve what is going on in `emb = C[Xb]`\n",
    "\n",
    "We have `C` a 27 x 10 feature vector matrix. `Xb` has examples each made of 3 letters. We want to pluck out the corresponding 3 x 10 feature vectors from `C` that encode the 3 letters/ indices in `Xb`. Each feature vector can obviously be used multiple times for encoding multiple examples.\n",
    "\n",
    "In `demb`, we now have the gradients to these features from each example. We want to route them to the correct feature in `C`. So, for each example in `Xb`, we want to select the `demb` gradients that correspond to it. Each example has 3 x 10 letters. These letters will fall into 27 possibilities in a `dC` matrix that is 27 x 10. If we see a letter's gradient from multiple examples, we should add them up.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dC = torch.zeros_like(C)\n",
    "for i in range(Xb.shape[0]): # go through each example\n",
    "    for j in range(Xb.shape[1]): # each index in the example\n",
    "        ix = Xb[i, j] # pluck index of each letter in the example\n",
    "        dC[ix] += demb[i, j] # demb[i,j] -> gets us the 10 gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My Hacky Vectorized Version**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Karpathy was saying he didn't know a better way to do this than for loops. I tried to do one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = torch.nn.functional.one_hot(Xb, num_classes=27).float()\n",
    "one_hot = one_hot.view(-1, 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([96, 27])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([96, 10])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demb_reshaped = demb.view(32*3, -1)\n",
    "demb_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dC2 = one_hot.T @ demb_reshaped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're done :))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "mO-8aqxK8PPw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnraw           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hprebn          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "embcat          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "emb             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "C               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "C2              | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: backprop through the whole thing manually,\n",
    "# backpropagating through exactly all of the variables\n",
    "# as they are defined in the forward pass above, one by one\n",
    "\n",
    "# -----------------\n",
    "# YOUR CODE HERE :)\n",
    "# -----------------\n",
    "\n",
    "\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)\n",
    "cmp('emb', demb, emb)\n",
    "cmp('C', dC, C)\n",
    "cmp('C2', dC2, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2: \n",
    "\n",
    "backprop through cross_entropy but all in one go. To complete this challenge look at the mathematical expression of the loss, take the derivative, simplify the expression, and just write it out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Entropy explained:\n",
    "\n",
    "Remeber how we computed the loss function:\n",
    "```\n",
    "logits = xenc @ W # predict log-counts\n",
    "\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "# probabilities for next character. These last 2 lines here are together called a 'softmax'\n",
    "\n",
    "p = probs[i, y] # pluck out the row for the current label\n",
    "logp = torch.log(p)\n",
    "nll = -logp\n",
    "nlls[i] = nll\n",
    "```\n",
    "\n",
    "We see from this, the loss for a single example is\n",
    "$$ L = -log(P_y) $$\n",
    "Where $P_y$ is a vector of probabilities representing the distribution of y across all possible categories of the output.\n",
    "\n",
    "Inside $P_y$ are individual probabilities $p_i$ where:\n",
    "\n",
    "$$p_i = \\frac{exp(z_i)}{\\sum_{j=1}^{C} exp(z_j)}$$\n",
    "\n",
    "Where $z_k$ is the logit of the correct class and $C$ is the number of possible classes. \n",
    "\n",
    "What we want is the derivative of the Loss w.r.t each logit output ie:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial z_i} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see an toy example of the computations for the steps above in the following table:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|      label   |      output   |   exp  |   softmax/probs |   one-hot/$y_i$ |   index |\n",
    "|:-------------:|:-------------:|:-----:|:---------:|:--------:|:-------:|\n",
    "|      a       | -4.89         |   0.01 |     0    |    0      |    0    |\n",
    "|      b       |  2.60         |  13.43 |     0.87 |    1      |    1    |\n",
    "|      c       |  0.59         |   1.81 |     0.12 |    0      |    2    |\n",
    "|      d       | -2.07         |   0.13 |     0.01 |    0      |    3    |\n",
    "|      e       | -4.57         |   0.01 |     0    |    0      |    4    |\n",
    "|              | 15.38         |   1.00 |          |           |         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our logits are $l_i$, we want $$\\frac{dLoss}{dl_i}$$ where $$p_i = \\frac{exp(l_k)}{\\sum_{j=1}^{C} exp(l_j)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Via chain rule this is:\n",
    "\n",
    "$$ \\frac{dLoss}{dp_i} * \\frac{dp_i}{dl_i} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{dp_i}{dl_i}$ is the derivative of the softmax function. Let's do that first: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p_i = \\frac{exp(l_k)}{\\sum_{j=1}^{C} exp(l_j)}$$\n",
    "\n",
    "There are 2 cases here:\n",
    "1. $k = i$ : The case for the right label\n",
    "2. $k \\neq i $ : The case for all other labels\n",
    "\n",
    "Remeber the Quotient rule:\n",
    "\n",
    "The derivative quotient rule states that for two differentiable functions, $u(x)$ and $v(x)$, the derivative of their quotient $\\frac{du(x)}{dv(x)}$ is given by:\n",
    "\n",
    "\n",
    "$$\\frac{d}{dx}\\left(\\frac{u(x)}{v(x)}\\right) = \\frac{v(x) \\cdot u'(x) - u(x) \\cdot v'(x)}{v(x)^2}$$\n",
    "\n",
    "Or what I like to remeber as: \n",
    "$$ \\frac{d{up}*down - d{down}*up}{(down)^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ddown = \\frac{\\sum_{j=1}^{C} exp(l_j)}{dl_i} = exp(l_i) $$\n",
    "\n",
    "where if the sum is $exp(l_i) + exp(l_k) + exp(l_a) ...$ etc, then one of the summed elements will always be the correct class $l_i$ whose derivative is itself $exp(l_i)$ and all else is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of $k = i$ this becomes:\n",
    "\n",
    "$$= \\frac{{\\frac{dexp(l_i)}{dl_i}}*\\sum - d{\\sum}*exp(l_i)}{(\\sum)^2}$$\n",
    "\n",
    "$$= \\frac{exp(l_i) * \\sum - exp(l_i) * exp(l_i)}{(\\sum)^2}$$\n",
    "\n",
    "$$= \\frac{exp(l_i)}{\\sum} - (\\frac{exp(l_i)}{\\sum})^2 $$\n",
    "$$= p_i - p_i^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case where $k \\neq 1$ this becomes\n",
    "\n",
    "$$= \\frac{{\\frac{dexp(l_k)}{dl_i}}*\\sum - d{\\sum}*exp(l_k)}{(\\sum)^2}$$\n",
    "$d\\sum$ we know = $exp(l_i)$\n",
    "$$= \\frac{ 0 * \\sum - exp(l_i) * exp(l_k)}{(\\sum)^2} $$\n",
    "$$= \\frac{- exp(l_i) * exp(l_k)}{(\\sum)^2}$$\n",
    "$$ = - \\frac{exp(l_i)}{\\sum} * \\frac{exp(l_k)}{\\sum} $$\n",
    "$$ = - p_i * p_k $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the derivative of softmax, let's do the Loss function: $$ \\frac{dLoss}{dl_i}$$ where $$ Loss = -log(P_y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall $$\\frac{d}{dx} log(x) = \\frac{1}{x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{d(-log(P_y))}{dl_i} = - \\frac{1}{P_y} * \\frac{dP_y}{dl_i} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know in the case of $y = i$ then $$\\frac{dP_y}{dl_i} = P_y - (P_y)^2 = P_y(1 - P_y)$$\n",
    "\n",
    "So we replace this term and we get:\n",
    "\n",
    "$$ \\frac{d(-log(P_y))}{dl_i} = - \\frac{1}{P_y} * P_y(1 - P_y) = P_y - 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if $ y \\neq i$ (such that y = some k; $y = k$) then $$\\frac{dP_y}{dl_i} = -P_y * P_k $$\n",
    "\n",
    "$$ \\frac{d(-log(P_k))}{dl_i} = - \\frac{1}{P_k} * -P_y * P_k = P_y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind:\n",
    "1. $P_y$ is a vector of probabilities produced by Softmax\n",
    "2. This is done for a single example. In our code, we work with batches of examples. The loss for a batch is the average loss over all the examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in code\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "ebLtYji_8PPw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3402700424194336 diff: 4.76837158203125e-07\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: backprop through cross_entropy but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the loss,\n",
    "# take the derivative, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# now:\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "-gCXbB4C8PPx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 6.984919309616089e-09\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "# -----------------\n",
    "# YOUR CODE HERE :)\n",
    "dlogits = None # TODO. my solution is 3 lines\n",
    "# -----------------\n",
    "\n",
    "dlogits = F.softmax(logits, dim=1) # each row is one example. Its sum will be sum across all 27 columns\n",
    "# then, at the correct positions (columns) (Yb), the derivative is P - 1\n",
    "dlogits[range(logits.shape[0]), Yb] -= 1 # we could replace logits.shape[0] with n which is the batch size\n",
    "# then, we remember loss computes averge loss: loss = -logprobs[range(n), Yb].mean(). We just computed the \n",
    "# gradient for the absolute loss for each example. We need now to scale it down to be for the average loss \n",
    "dlogits /= n\n",
    "\n",
    "cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "hd-MkhB68PPy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff: tensor(9.5367e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3: backprop through batchnorm but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
    "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
    "# BatchNorm paper: https://arxiv.org/abs/1502.03167\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff**2\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# now:\n",
    "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
    "print('max diff:', (hpreact_fast - hpreact).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "POdeZSKT8PPy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprebn          | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-09\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "# before we had:\n",
    "# dbnraw = bngain * dhpreact\n",
    "# dbndiff = bnvar_inv * dbnraw\n",
    "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "# dbndiff += (2*bndiff) * dbndiff2\n",
    "# dhprebn = dbndiff.clone()\n",
    "# dbnmeani = (-dbndiff).sum(0)\n",
    "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "\n",
    "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
    "# (you'll also need to use some of the variables from the forward pass up above)\n",
    "\n",
    "# -----------------\n",
    "# YOUR CODE HERE :)\n",
    "dhprebn = None # TODO. my solution is 1 (long) line\n",
    "dhprebn = (bngain * bnvar_inv) * ( dhpreact - dhpreact.sum(0)/n -  (bnraw * (dhpreact*bnraw).sum(0))/(n-1) )\n",
    "#dhprebn = bngain*bnvar_inv/n * ((n*dhpreact - dhpreact.sum(0)) - n/(n-1) * bnraw * (dhpreact*bnraw).sum(0))\n",
    "# -----------------\n",
    "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]), torch.Size([32, 64]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hprebn.shape, dhprebn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wPy8DhqB8PPz"
   },
   "outputs": [],
   "source": [
    "# Exercise 4: putting it all together!\n",
    "# Train the MLP neural net with your own backward pass\n",
    "\n",
    "# init\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "n = batch_size # convenience\n",
    "lossi = []\n",
    "\n",
    "# use this context manager for efficiency once your backward pass is written (TODO)\n",
    "#with torch.no_grad():\n",
    "\n",
    "# kick off optimization\n",
    "for i in range(max_steps):\n",
    "\n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "\n",
    "  # forward pass\n",
    "  emb = C[Xb] # embed the characters into vectors\n",
    "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "  # Linear layer\n",
    "  hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "  # BatchNorm layer\n",
    "  # -------------------------------------------------------------\n",
    "  bnmean = hprebn.mean(0, keepdim=True)\n",
    "  bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
    "  bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "  bnraw = (hprebn - bnmean) * bnvar_inv\n",
    "  hpreact = bngain * bnraw + bnbias\n",
    "  # -------------------------------------------------------------\n",
    "  # Non-linearity\n",
    "  h = torch.tanh(hpreact) # hidden layer\n",
    "  logits = h @ W2 + b2 # output layer\n",
    "  loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "  # backward pass\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward() # use this for correctness comparisons, delete it later!\n",
    "\n",
    "  # manual backprop! #swole_doge_meme\n",
    "  # -----------------\n",
    "  # YOUR CODE HERE :)\n",
    "  dC, dW1, db1, dW2, db2, dbngain, dbnbias = None, None, None, None, None, None, None\n",
    "  grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "  # -----------------\n",
    "\n",
    "  # update\n",
    "  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "  for p, grad in zip(parameters, grads):\n",
    "    p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
    "    #p.data += -lr * grad # new way of swole doge TODO: enable\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())\n",
    "\n",
    "  if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZEpI0hMW8PPz"
   },
   "outputs": [],
   "source": [
    "# useful for checking your gradients\n",
    "# for p,g in zip(parameters, grads):\n",
    "#   cmp(str(tuple(p.shape)), g, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KImLWNoh8PP0"
   },
   "outputs": [],
   "source": [
    "# calibrate the batch norm at the end of training\n",
    "\n",
    "with torch.no_grad():\n",
    "  # pass the training set through\n",
    "  emb = C[Xtr]\n",
    "  embcat = emb.view(emb.shape[0], -1)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  # measure the mean/std over the entire training set\n",
    "  bnmean = hpreact.mean(0, keepdim=True)\n",
    "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6aFnP_Zc8PP0"
   },
   "outputs": [],
   "source": [
    "# evaluate train and val loss\n",
    "\n",
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "  logits = h @ W2 + b2 # (N, vocab_size)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "esWqmhyj8PP1"
   },
   "outputs": [],
   "source": [
    "# I achieved:\n",
    "# train 2.0718822479248047\n",
    "# val 2.1162495613098145"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xHeQNv3s8PP1"
   },
   "outputs": [],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "\n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      # forward pass\n",
    "      emb = C[torch.tensor([context])] # (1,block_size,d)\n",
    "      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "      hpreact = embcat @ W1 + b1\n",
    "      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "      logits = h @ W2 + b2 # (N, vocab_size)\n",
    "      # sample\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      if ix == 0:\n",
    "        break\n",
    "\n",
    "    print(''.join(itos[i] for i in out))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
